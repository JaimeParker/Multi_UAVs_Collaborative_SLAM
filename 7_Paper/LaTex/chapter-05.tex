\renewcommand{\baselinestretch}{1.5}
\fontsize{12pt}{13pt}\selectfont

\chapter{实验与评估} \label{experiment}

进行的实验主要有两部分，真机的SLAM实验和地图融合实验。

\section{真机配置与实验}

真机使用单机实验。

真机搭载英特尔T265摄像头，其特点是带两个鱼眼镜头和惯性测量单元，并且视觉SLAM可以运行在其特有的视觉处理单元上。其在电脑上需要一定配置，安装英特尔的一些以来库文件和realsense-viewer。

除摄像头外，真机还搭载Jetson Xavier NX计算机，体积较小且接口丰富。需要注意的是，无人机在地面时，可以使用显示器和鼠标设备对其进行操作，运行ROS节点；但当无人机在空中时，没有显示信息和鼠标的操作输入，因此需要使用远程控制，在这里使用了No Machine软件进行远程控制。

实验内容如下：

在真机实验之前，首先使用T265相机中的单目相机进行室内的建图实验；使用单目相机进行ORB-SLAM2的关键是初始化，这是由单目相机的特性决定的；由于其拍摄到的图片不具有深度信息，因此需要依靠三角化恢复深度。而三角化的特点是，两帧之间的平移向量不能为零，否则解出的旋转矩阵也为零；因此单目相机的初始化方法是使相机在相同场景中不断左右或上下平移。初始化成功后，ORB-SLAM2进入跟踪模式，需要注意当进入陌生场景（比如直角转弯）时，移动应尽量缓慢，否则很容易跟丢，进入重定位模式。
但进入重定位模式后，只要将视角对准经历过的场景，一般又能重新恢复跟踪。

共进行两个场景的稀疏点云建图，如图\ref{result1}，图\ref{result2}所示：

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{0.45\columnwidth} %小于1/n如果使用n张图片
		\centering
		\includegraphics[width=1.0\columnwidth]{ou1-1.png}
		\caption{教东B一楼楼道}
		\label{result1}
	\end{minipage}
	\begin{minipage}[t]{0.45\columnwidth}
		\centering
		\includegraphics[width=1.0\columnwidth]{ou3-1.png}
		\caption{教东C二楼楼道}
		\label{result2}
	\end{minipage}
\end{figure}


\section{地图融合实验}

在教学楼楼道场景下，使用手持相机的方式，沿楼道行走，完成对一段楼道的建图；同时准备另一台相机，沿不同的路线行进，完成对同一段楼道的建图；两段视频应该拥有一部分重合的场景和一部分不重合的场景，用以验证地图融合的效果。

需要注意的是，使用单目视觉SLAM时，初始化是一个重要的步骤，一般采用在一个场景左右平移的方法完成初始化，之后才能进行跟踪进程，否则一段视频中将会有大量的时间用于初始化，这也意味着选取视频的质量十分重要。

在进行SLAM进程前，需要对相机进行标定，可以采用张正友标定法；需要注意的是，相机的拍照模式和录像模式，其内参矩阵可能不同；对于使用视觉SLAM的相机，建议采用动态标定法。

最后，使用预先录制的视频验证SLAM算法有两个方式：

\begin{enumerate}
	\item 将视频转换成KITTI或TUM数据集的格式；通过观察这些数
	据集的格式可以看出，其一般是灰度图加深度信息，并且配有时间戳，这样的转换可能有一些复杂。转换完成后，使用ROS的例子中自带的对于特定传感器和特定数据集的可执行文件运行整个SLAM进程即可。
	\item 较为简单的方式是将其转换为ROS的rosbag，rosbag可以记录特定话题的信息；对于视频而言，即记录不同时间戳下矩阵各点的三通道RGB值，对于灰度视频则更为方便；其用法是使用ROS的cv bridge完成一些MP4格式视频到rosbag包的转换。
\end{enumerate}

完成MP4格式到rosbag的部分代码如下：

\begin{minted}[fontsize=\small]{cpp}
// set wait key = 1000/fps
cv::waitKey(int(1000/fps));

std_msgs::Header header;
header.frame_id = "frame";
header.stamp = ros::Time::now();

sensor_msgs::ImagePtr ImageMsg = cv_bridge::CvImage(header, 
    "bgr8", frame).toImageMsg();

if(FrameNum % 2 == 0){
    my_bag.write(topicName, ros::Time::now(), ImageMsg);
    cout << "recording frames:" << frameNum << endl;
}
\end{minted}                

其主要逻辑是，将每一帧的信息转换成sensor\_msg类型的消息，之后写入到rosbag中。这里需要注意waitKey函数，其含义为循环等待，单位是1/1000秒，因此1000/fps即为按照fps帧率，每一帧需要等待的时间；该参数控制了视频的帧率，从而控制了rosbag包的长度。topicName为录制的话题名，已经提前定义好。

实验在教东B教室进行，从后门和窗户两个方向相向而行，录制两段视频，尽量保证两段视频中出现重叠场景，以方便地图融合；

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{0.45\columnwidth} %小于1/n如果使用n张图片
		\centering
		\includegraphics[width=1.0\columnwidth]{ccm4.png}
		\caption{教室特征点匹配示例}
		\label{fig5-1}
	\end{minipage}
	\begin{minipage}[t]{0.45\columnwidth}
		\centering
		\includegraphics[width=1.0\columnwidth]{ccm2.png}
		\caption{地图融合}
		\label{fig5-2}
	\end{minipage}
\end{figure}

可以看出，基本完成了建图和地图融合的任务。