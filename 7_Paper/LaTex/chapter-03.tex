\chapter{SLAM系统设计} \label{System Overview}
本章节主要介绍SLAM的概念和原理、一个基本SLAM系统的框架、两个优秀的开源SLAM框架和地图融合算法。

\section{SLAM系统}
同时定位与建图（SLAM，simultaneous localization and mapping）技术，其希望是机器人在对环境和自身所处在环境中的位置未知的情况下，在反复的运动过程中不断观测到的地图特征完成自身位置的定位和姿态的确定，之后再根据自身位置对环境构建增量式的地图，从而达到同时定位与建图的目的。


\subsection{SLAM的分类} \label{3.1.1}
SLAM主要分为视觉SLAM、激光SLAM、融合SLAM和新颖SLAM。


对于视觉SLAM，即用相机完成同时定位与建图的任务。由于相机造价相对较低、电量消耗相对较少、能够获取环境的大量信息，因此相机成为了完成定位与建图任务常用的传感器。视觉SLAM主要有五个步骤，传感器信息读取、视觉里程计（Visual Odometry）、后端优化（Optimization）、回环检测（Loop Closing）、建图（Mapping）[6] 。对于静态、刚体、光照变化不明显、且没有过多人为干扰的场景，视觉SLAM技术已经十分成熟。当前比较好的方案有ORB-SLAM；其在对特征点的描述上做了很大创新，相比于SIFT（尺度不变特征变换，Scale-invariant feature transform）的大计算量和对GPU的特殊需求、FAST关键点描述没有描述子的缺点，ORB改进了FAST的检测子，为其增加了方向性，并且采用了二进制描述子BRIEF（Binary Robust Independent Elementary Feature）[5] 。


对于激光SLAM，主要有两种传感器，单线束激光雷达和多线束激光雷达；单线束激光雷达即2D雷达，2D激光雷达的扫描范围比较单一，角度有限，因此比较适合仅平面运动的机器人的定位与建图，对应的经典算法如GMapping；多线束雷达即3D雷达，其获取的信息包含距离和角度，能够还原出目标的三维点云，且不受光照影响，缺点是造价比较昂贵且易受不良天气影响[7] ，对应的经典算法如谷歌提出的Cartographer。


对于融合SLAM，常见的有视觉和惯性的融合，即相机+IMU（inertial measurement unit，惯性测量单元，包含加速度计和角加速度计）等的多传感器融合；IMU的工作原理是对加速度的积分、初始速度和起始位置进行混合运算，得到运动轨迹和位姿。但是其容易产生漂移（Drift），并且这种累积误差会随时间增加[8] 。


对于VIO（视觉惯性里程计），即上文提到的由相机和惯性测量单元组成的融合传感器，根据融合的框架可以分为松耦合和紧耦合两种。松耦合中对相机关键帧数据的视觉运动估计和对IMU测量数据的运动估计是两个独立的模块，计算时互不干涉；计算完成后将其轨迹结果按一定的方法进行融合。紧耦合则是共同使用相机视觉数据和惯导运动估计数据，共同完成对一组变量的估计；因此其算法更加复杂，且传感器之间的噪声也会相互影响，但是具有更好的效果，也是目前阶段研究的重点方向。这方向上好的方案有VINS-fusion[9]。


对于新颖SLAM，比如语义SLAM；使用神经网络的语义分割、目标检测方法，从图像到位姿，使用其语义分割的结果来完成点云地图的建立和场景识别。语义SLAM能够探测出图像中的各个物体，并且能得到在图像中的位置，可以节省传统的人工标定物品的成本，方便机器人的自主理解能力和简便的人机交互[11] 。


\subsection{成像原理及相机参数} \label{3.1.2}
在各种SLAM中，视觉SLAM由于其传感器（光学相机）造价较低的原因，成为了SLAM中最常用的方式，要了解使用相机的视觉SLAM的原理，首先需要了解相机的成像原理及其参数。

如图\ref{fig4}，可以用小孔成像的原理简单地解释针孔相机的模型：
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.6\textwidth]{camera.png}
	\caption{针孔相机模型} 
	\label{fig4}
\end{figure}

$Oxy$平面为相机光心（垂直主光轴）所在的平面，称其为相机平面，对应的$O-x-y-z$坐标系即为相机坐标系；$O'x'y'$平面为物理成像平面，$\bar{OO'}$的长度为焦距$f$；在现实世界中有一点$P$，设其在相机坐标系下的坐标为$[X, Y, Z]^T$，其经过小孔$O$投影后，在相机坐标系下落在像素平面上的坐标为$[X', Y', Z']^T$。

理论下，小孔成像为倒立的实像，但在实际的相机中，成像被人为旋转，成正立的像，因此不考虑坐标系正负号的影响，由相似三角形关系，有：

$$
\frac{Z}{f}=\frac{X}{X'}=\frac{Y}{Y'}
$$

在此基础上，定义像素坐标系。像素坐标系为二维坐标系，在物理成像平面上；像素坐标系的原点位于图像的左上角，横轴为$u$轴，向右与$x$轴平行，纵轴为$v$轴，向下与$y$轴平行，则可以得到像素坐标与$P'$坐标的关系为：

$$
\begin{cases}
u=\alpha X'+c_x=\alpha f \cfrac{X}{Z}+c_x \\
v=\beta Y' +c_y=\beta f  \cfrac{Y}{Z}+c_y \\
\end{cases}
$$

其中，$\alpha$和$\beta$为横轴和纵轴的缩放倍数，$c_x$为图像横向像素的一半，$c_y$为图像纵向像素的一半。令$f_x=\alpha f$，$f_y=\beta f$，将像素坐标系下的坐标转换为齐次坐标：

$$
\begin{bmatrix}
u\\v\\1
\end{bmatrix}=
\frac{1}{Z}
\begin{bmatrix}
f_x & 0 & c_x\\
0   & f_y & c_y\\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
X\\Y\\Z
\end{bmatrix}=
\frac{1}{Z} \bf{KP}
$$

则得到了相机的内参矩阵（Camera Intrinsics）$\bf{K}$。通常情况下，对于焦距固定的相机（或定焦镜头），其出厂之后内参矩阵是固定的；如果无法从厂家得到相机的内参，可以使用标定的方法获得相机的内参矩阵，常见的标定算法有张正友标定法。

除相机的内参外，还有相机外参（Camera Extrinsics）的定义；相机的外参由其旋转矩阵$\bf{R}$和平移向量$\bf{t}$构成。对于$P$点而言，其在相机坐标系（像素坐标平面）下的坐标应为其在世界坐标系下的坐标根据相机相对于世界坐标系的位姿所变换得到的，相机的位姿即由其外参决定，则世界坐标系下$P$点的坐标$\bf{P_w}$在相机坐标系下的坐标为：

$$
Z
\begin{bmatrix}
u\\v\\1
\end{bmatrix}=
\bf{K(RP_w+t)}
$$

\subsection{视觉SLAM的基本步骤} \label{3.1.3}

经典的视觉SLAM框架如图\ref{fig5}所示：

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.6\textwidth]{SLAM.png}
	\caption{经典视觉SLAM框架} 
	\label{fig5}
\end{figure}

经典视觉SLAM流程包括以下基本步骤：
\begin{enumerate}
	\item 
	传感器信息读取。主要为相机图像的读取以及一些预处理步骤，在不同的视觉SLAM算法中，可能还涉及惯性测量元件信息的读取和预处理。
	\item 
	前端视觉里程计（Visual Odometry）。视觉里程计的功能是从相邻的几帧图像之中，根据对极几何或其他约束，得到相机的运动；并且通过记录地图点（路标）与相机的相对位置，构建局部地图。视觉里程计是SLAM的关键，其基本完成了同时定位与建图的任务
	\item 
	后端（非线性）优化（Optimization）。
	\item 
	回环检测（Loop Closure Detection）。
	\item 
	建图（Mapping）。
\end{enumerate}


\section{ORB-SLAM2}

\subsection{ORB特征点及描述子} \label{3.2.1}
\subsection{ORB-SLAM2的主要进程} \label{3.2.2}


\section{CCM-SLAM}

\subsection{CCM-SLAM的结构} \label{3.3.1}
\subsection{Client与Server机制} \label{3.3.2}


\section{多机协同及地图融合方案}

\subsection{算法原理} \label{3.4.1}
\subsection{编程实现} \label{3.4.2}